---
title: 시선 추적
description: HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability to use information about what the user is looking at.
author: sostel
ms.author: sostel
ms.date: 10/29/2019
ms.topic: article
keywords: Eye tracking, mixed reality, input, eye-gaze, calibration
ms.openlocfilehash: 1f3699330fb4879258693b6959724441bd838d98
ms.sourcegitcommit: 4d43a8f40e3132605cee9ece9229e67d985db645
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 11/26/2019
ms.locfileid: "74491146"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="9baa6-104">HoloLens 2의 시선 추적</span><span class="sxs-lookup"><span data-stu-id="9baa6-104">Eye tracking on HoloLens 2</span></span>

![Eye tracking demo in MRTK](images/mrtk_et_scenemenu.jpg)

<span data-ttu-id="9baa6-106">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability to use information about what the user is looking at.</span><span class="sxs-lookup"><span data-stu-id="9baa6-106">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability to use information about what the user is looking at.</span></span> <span data-ttu-id="9baa6-107">This page explains how developers can benefit from eye tracking for various use cases, as well as what to look for when designing eye-gaze-based user interactions.</span><span class="sxs-lookup"><span data-stu-id="9baa6-107">This page explains how developers can benefit from eye tracking for various use cases, as well as what to look for when designing eye-gaze-based user interactions.</span></span> 

<span data-ttu-id="9baa6-108">Eye tracking API has been designed with a user’s privacy in mind, avoiding passing any identifiable information, particularly any biometrics.</span><span class="sxs-lookup"><span data-stu-id="9baa6-108">Eye tracking API has been designed with a user’s privacy in mind, avoiding passing any identifiable information, particularly any biometrics.</span></span> <span data-ttu-id="9baa6-109">For eye-tracking capable applications, the user needs to grant app permission to use eye tracking information.</span><span class="sxs-lookup"><span data-stu-id="9baa6-109">For eye-tracking capable applications, the user needs to grant app permission to use eye tracking information.</span></span> 


### <a name="device-support"></a><span data-ttu-id="9baa6-110">장치 지원</span><span class="sxs-lookup"><span data-stu-id="9baa6-110">Device support</span></span>
<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><span data-ttu-id="9baa6-111"><strong>Feature</strong></span><span class="sxs-lookup"><span data-stu-id="9baa6-111"><strong>Feature</strong></span></span></td>
     <td><span data-ttu-id="9baa6-112"><a href="hololens-hardware-details.md"><strong>HoloLens(1세대)</strong></a></span><span class="sxs-lookup"><span data-stu-id="9baa6-112"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
     <td><span data-ttu-id="9baa6-113"><a href="https://docs.microsoft.com/hololens/hololens2-hardware"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="9baa6-113"><a href="https://docs.microsoft.com/hololens/hololens2-hardware"><strong>HoloLens 2</strong></span></span></td>
     <td><span data-ttu-id="9baa6-114"><a href="immersive-headset-hardware-details.md"><strong>몰입형 헤드셋</strong></a></span><span class="sxs-lookup"><span data-stu-id="9baa6-114"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
</tr>
<tr>
     <td><span data-ttu-id="9baa6-115">Eye-gaze</span><span class="sxs-lookup"><span data-stu-id="9baa6-115">Eye-gaze</span></span></td>
     <td>❌</td>
     <td><span data-ttu-id="9baa6-116">✔️</span><span class="sxs-lookup"><span data-stu-id="9baa6-116">✔️</span></span></td>
     <td>❌</td>
</tr>
</table>

<br>

## <a name="calibration"></a><span data-ttu-id="9baa6-117">Calibration</span><span class="sxs-lookup"><span data-stu-id="9baa6-117">Calibration</span></span> 
<span data-ttu-id="9baa6-118">For eye tracking to work accurately, each user is required to go through an [eye tracking user calibration](calibration.md) for which the user has to look at a set of holographic targets.</span><span class="sxs-lookup"><span data-stu-id="9baa6-118">For eye tracking to work accurately, each user is required to go through an [eye tracking user calibration](calibration.md) for which the user has to look at a set of holographic targets.</span></span> <span data-ttu-id="9baa6-119">This allows the device to adjust the system for a more comfortable and higher quality viewing experience for the user and to ensure accurate eye tracking at the same time.</span><span class="sxs-lookup"><span data-stu-id="9baa6-119">This allows the device to adjust the system for a more comfortable and higher quality viewing experience for the user and to ensure accurate eye tracking at the same time.</span></span> 

<span data-ttu-id="9baa6-120">Eye tracking should work for most users, but there are rare cases in which a user might not be able to calibrate successfully.</span><span class="sxs-lookup"><span data-stu-id="9baa6-120">Eye tracking should work for most users, but there are rare cases in which a user might not be able to calibrate successfully.</span></span> <span data-ttu-id="9baa6-121">Calibration might fail for various reasons, including but not limited to:</span><span class="sxs-lookup"><span data-stu-id="9baa6-121">Calibration might fail for various reasons, including but not limited to:</span></span> 
* <span data-ttu-id="9baa6-122">The user previously opted out of the calibration process</span><span class="sxs-lookup"><span data-stu-id="9baa6-122">The user previously opted out of the calibration process</span></span>
* <span data-ttu-id="9baa6-123">The user got distracted and didn't follow the calibration targets</span><span class="sxs-lookup"><span data-stu-id="9baa6-123">The user got distracted and didn't follow the calibration targets</span></span>
* <span data-ttu-id="9baa6-124">The user has certain types of contact lenses and glasses which the system doesn't yet support</span><span class="sxs-lookup"><span data-stu-id="9baa6-124">The user has certain types of contact lenses and glasses which the system doesn't yet support</span></span> 
* <span data-ttu-id="9baa6-125">The user has certain eye physiology, eye conditions or had eye surgery which the system doesn't yet support</span><span class="sxs-lookup"><span data-stu-id="9baa6-125">The user has certain eye physiology, eye conditions or had eye surgery which the system doesn't yet support</span></span>  
* <span data-ttu-id="9baa6-126">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes</span><span class="sxs-lookup"><span data-stu-id="9baa6-126">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes</span></span>

<span data-ttu-id="9baa6-127">Developers should make sure to provide adequate support for users for whom eye tracking data may not be available (who are not able to calibrate successfully).</span><span class="sxs-lookup"><span data-stu-id="9baa6-127">Developers should make sure to provide adequate support for users for whom eye tracking data may not be available (who are not able to calibrate successfully).</span></span> <span data-ttu-id="9baa6-128">We have provided recommendations for fallback solutions in the section at the bottom of this page.</span><span class="sxs-lookup"><span data-stu-id="9baa6-128">We have provided recommendations for fallback solutions in the section at the bottom of this page.</span></span> 

<span data-ttu-id="9baa6-129">To learn more about the calibration and about how to ensure a smooth experience, please check our [eye tracking user calibration](calibration.md) page.</span><span class="sxs-lookup"><span data-stu-id="9baa6-129">To learn more about the calibration and about how to ensure a smooth experience, please check our [eye tracking user calibration](calibration.md) page.</span></span>

<br>

## <a name="available-eye-tracking-data"></a><span data-ttu-id="9baa6-130">Available eye tracking data</span><span class="sxs-lookup"><span data-stu-id="9baa6-130">Available eye tracking data</span></span>
<span data-ttu-id="9baa6-131">Before going into detail about specific use cases for eye-gaze input, we want to briefly point out the capabilities that the HoloLens 2 [Eye Tracking API](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose) provides.</span><span class="sxs-lookup"><span data-stu-id="9baa6-131">Before going into detail about specific use cases for eye-gaze input, we want to briefly point out the capabilities that the HoloLens 2 [Eye Tracking API](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose) provides.</span></span> <span data-ttu-id="9baa6-132">Developers get access to a single eye-gaze ray (gaze origin and direction) at approximately _30 FPS (30 Hz)_ .</span><span class="sxs-lookup"><span data-stu-id="9baa6-132">Developers get access to a single eye-gaze ray (gaze origin and direction) at approximately _30 FPS (30 Hz)_.</span></span>
<span data-ttu-id="9baa6-133">For more detailed information about how to access eye tracking data, please refer to our developer guides for using [eye-gaze in DirectX](gaze-in-directx.md) and [eye-gaze in Unity](https://aka.ms/mrtk-eyes).</span><span class="sxs-lookup"><span data-stu-id="9baa6-133">For more detailed information about how to access eye tracking data, please refer to our developer guides for using [eye-gaze in DirectX](gaze-in-directx.md) and [eye-gaze in Unity](https://aka.ms/mrtk-eyes).</span></span>

<span data-ttu-id="9baa6-134">The predicted eye-gaze is approximately within 1.5 degrees in visual angle around the actual target (see the illustration below).</span><span class="sxs-lookup"><span data-stu-id="9baa6-134">The predicted eye-gaze is approximately within 1.5 degrees in visual angle around the actual target (see the illustration below).</span></span> <span data-ttu-id="9baa6-135">As slight imprecisions are expected, developers should plan for some margin around this lower-bound value (e.g., 2.0-3.0 degrees may result in a much more comfortable experience).</span><span class="sxs-lookup"><span data-stu-id="9baa6-135">As slight imprecisions are expected, developers should plan for some margin around this lower-bound value (e.g., 2.0-3.0 degrees may result in a much more comfortable experience).</span></span> <span data-ttu-id="9baa6-136">We will discuss how to address the selection of small targets in more detail below.</span><span class="sxs-lookup"><span data-stu-id="9baa6-136">We will discuss how to address the selection of small targets in more detail below.</span></span> <span data-ttu-id="9baa6-137">시선 추적이 정확히 작동하려면 각 사용자가 시선 추적 사용자 보정을 진행해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-137">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="9baa6-138">![2m 거리에서 최적 대상 크기](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="9baa6-138">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="9baa6-139">*Optimal target size at a 2-meter distance*</span><span class="sxs-lookup"><span data-stu-id="9baa6-139">*Optimal target size at a 2-meter distance*</span></span>

<br>

## <a name="use-cases"></a><span data-ttu-id="9baa6-140">사용 사례</span><span class="sxs-lookup"><span data-stu-id="9baa6-140">Use cases</span></span>
<span data-ttu-id="9baa6-141">시선 추적을 사용하여 애플리케이션에서는 사용자가 실시간으로 보는 곳을 추적할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-141">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="9baa6-142">The following use cases describe some interactions that are possible with eye tracking on HoloLens 2 in mixed reality.</span><span class="sxs-lookup"><span data-stu-id="9baa6-142">The following use cases describe some interactions that are possible with eye tracking on HoloLens 2 in mixed reality.</span></span>
<span data-ttu-id="9baa6-143">Please note that these use cases are not yet part of the Holographic Shell experience (i.e., the interface that you see when you start up your HoloLens 2).</span><span class="sxs-lookup"><span data-stu-id="9baa6-143">Please note that these use cases are not yet part of the Holographic Shell experience (i.e., the interface that you see when you start up your HoloLens 2).</span></span>
<span data-ttu-id="9baa6-144">You can try some of them in the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html), which provides several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections, as well as automatically scrolling through text based on what the user looks at.</span><span class="sxs-lookup"><span data-stu-id="9baa6-144">You can try some of them in the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html), which provides several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections, as well as automatically scrolling through text based on what the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="9baa6-145">사용자 의도</span><span class="sxs-lookup"><span data-stu-id="9baa6-145">User intent</span></span>    
<span data-ttu-id="9baa6-146">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span><span class="sxs-lookup"><span data-stu-id="9baa6-146">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="9baa6-147">이러한 컨텍스트를 다양한 작업에서 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-147">This can be used for various tasks.</span></span>
<span data-ttu-id="9baa6-148">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying *"select"* (also see [gaze and commit](gaze-and-commit.md)) or *"put this..."* , then looking over to where the user wants to place the hologram and say *"...there"* .</span><span class="sxs-lookup"><span data-stu-id="9baa6-148">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying *"select"* (also see [gaze and commit](gaze-and-commit.md)) or *"put this..."*, then looking over to where the user wants to place the hologram and say *"...there"*.</span></span> <span data-ttu-id="9baa6-149">이에 대한 예는 [Mixed Reality Toolkit - 시선 지원 대상 선택](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) 및 [Mixed Reality Toolkit - 시선 지원 대상 위치 지정](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)에서 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-149">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="9baa6-150">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span><span class="sxs-lookup"><span data-stu-id="9baa6-150">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="9baa6-151">For instance, virtual agents might adapt available options and their behavior, based on currently viewed content.</span><span class="sxs-lookup"><span data-stu-id="9baa6-151">For instance, virtual agents might adapt available options and their behavior, based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="9baa6-152">암시적 작업</span><span class="sxs-lookup"><span data-stu-id="9baa6-152">Implicit actions</span></span>
<span data-ttu-id="9baa6-153">암시적 작업의 범주는 사용자 의도와 밀접하게 관련되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-153">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="9baa6-154">The idea is that holograms or user interface elements react in an instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user can read a long text which automatically starts scrolling once the user gets to the bottom of the textbox to keep the user in the flow of reading, without lifting a finger.</span><span class="sxs-lookup"><span data-stu-id="9baa6-154">The idea is that holograms or user interface elements react in an instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user can read a long text which automatically starts scrolling once the user gets to the bottom of the textbox to keep the user in the flow of reading, without lifting a finger.</span></span>  
<span data-ttu-id="9baa6-155">A key aspect of this is that the scrolling speed adapts to the reading speed of the user.</span><span class="sxs-lookup"><span data-stu-id="9baa6-155">A key aspect of this is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="9baa6-156">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused on.</span><span class="sxs-lookup"><span data-stu-id="9baa6-156">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused on.</span></span> <span data-ttu-id="9baa6-157">Triggering and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span><span class="sxs-lookup"><span data-stu-id="9baa6-157">Triggering and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span></span> <span data-ttu-id="9baa6-158">We will talk about these design considerations in more detail below.</span><span class="sxs-lookup"><span data-stu-id="9baa6-158">We will talk about these design considerations in more detail below.</span></span> <span data-ttu-id="9baa6-159">Once zoomed in, the user can smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span><span class="sxs-lookup"><span data-stu-id="9baa6-159">Once zoomed in, the user can smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span></span>
<span data-ttu-id="9baa6-160">이러한 유형의 상호 작용을 나타내는 데모 예제는 [Mixed Reality Toolkit - 시선 지원 탐색](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) 샘플에서 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-160">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="9baa6-161">_암시적 작업_의 추가 사용 사례로는 다음이 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-161">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="9baa6-162">**Smart notifications:** Ever get annoyed by notifications popping up right where you are looking?</span><span class="sxs-lookup"><span data-stu-id="9baa6-162">**Smart notifications:** Ever get annoyed by notifications popping up right where you are looking?</span></span> <span data-ttu-id="9baa6-163">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span><span class="sxs-lookup"><span data-stu-id="9baa6-163">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span></span> <span data-ttu-id="9baa6-164">This limits distractions and automatically dismisses them once the user is finished reading.</span><span class="sxs-lookup"><span data-stu-id="9baa6-164">This limits distractions and automatically dismisses them once the user is finished reading.</span></span> 
- <span data-ttu-id="9baa6-165">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span><span class="sxs-lookup"><span data-stu-id="9baa6-165">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span></span> <span data-ttu-id="9baa6-166">This can range from slightly glowing UI elements, a slowly blooming flower to a virtual dog starting to look back at the user and wagging its tail.</span><span class="sxs-lookup"><span data-stu-id="9baa6-166">This can range from slightly glowing UI elements, a slowly blooming flower to a virtual dog starting to look back at the user and wagging its tail.</span></span> <span data-ttu-id="9baa6-167">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span><span class="sxs-lookup"><span data-stu-id="9baa6-167">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="9baa6-168">주의 추적</span><span class="sxs-lookup"><span data-stu-id="9baa6-168">Attention tracking</span></span>   
<span data-ttu-id="9baa6-169">Information on where or what users look at can be an immensely powerful tool.</span><span class="sxs-lookup"><span data-stu-id="9baa6-169">Information on where or what users look at can be an immensely powerful tool.</span></span> <span data-ttu-id="9baa6-170">It can help assess usability of designs and identify problems in workflows to make them more efficient.</span><span class="sxs-lookup"><span data-stu-id="9baa6-170">It can help assess usability of designs and identify problems in workflows to make them more efficient.</span></span>
<span data-ttu-id="9baa6-171">Eye tracking visualization and analytics are a common practice in various application areas.</span><span class="sxs-lookup"><span data-stu-id="9baa6-171">Eye tracking visualization and analytics are a common practice in various application areas.</span></span> <span data-ttu-id="9baa6-172">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span><span class="sxs-lookup"><span data-stu-id="9baa6-172">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span></span> <span data-ttu-id="9baa6-173">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and how to visualize them.</span><span class="sxs-lookup"><span data-stu-id="9baa6-173">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and how to visualize them.</span></span>
<span data-ttu-id="9baa6-174">Microsoft is dedicated to facilitating innovation while ensuring that users have an informed and transparent experience with how their eye tracking information is used.</span><span class="sxs-lookup"><span data-stu-id="9baa6-174">Microsoft is dedicated to facilitating innovation while ensuring that users have an informed and transparent experience with how their eye tracking information is used.</span></span>  <span data-ttu-id="9baa6-175">We will work with our developers and UX teams to provide guidance for third parties to ensure that experiences are centered around the user.</span><span class="sxs-lookup"><span data-stu-id="9baa6-175">We will work with our developers and UX teams to provide guidance for third parties to ensure that experiences are centered around the user.</span></span>  


<span data-ttu-id="9baa6-176">이 영역의 다른 응용 분야로 다음이 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-176">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="9baa6-177">**Remote eye-gaze visualization:** Remote eye-gaze visualizations: Visualize what remote collaborators are looking at, to be able to provide immediate feedback and facilitate more accurate information processing.</span><span class="sxs-lookup"><span data-stu-id="9baa6-177">**Remote eye-gaze visualization:** Remote eye-gaze visualizations: Visualize what remote collaborators are looking at, to be able to provide immediate feedback and facilitate more accurate information processing.</span></span>
-   <span data-ttu-id="9baa6-178">**User research studies:** Attention tracking can help researchers get more insights into how users perceive and engage with the natural environment, without interfering, to design more instinctual human-computer-interactions.</span><span class="sxs-lookup"><span data-stu-id="9baa6-178">**User research studies:** Attention tracking can help researchers get more insights into how users perceive and engage with the natural environment, without interfering, to design more instinctual human-computer-interactions.</span></span> <span data-ttu-id="9baa6-179">Eye tracking can provide information that is not directly articulated by participants in the study, which otherwise might be easily missed by the researcher.</span><span class="sxs-lookup"><span data-stu-id="9baa6-179">Eye tracking can provide information that is not directly articulated by participants in the study, which otherwise might be easily missed by the researcher.</span></span> 
-   <span data-ttu-id="9baa6-180">**Training and performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span><span class="sxs-lookup"><span data-stu-id="9baa6-180">**Training and performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span> <span data-ttu-id="9baa6-181">Eye tracking can provide natural, real-time and objective information to help improve training, productivity, and safety in the workplace.</span><span class="sxs-lookup"><span data-stu-id="9baa6-181">Eye tracking can provide natural, real-time and objective information to help improve training, productivity, and safety in the workplace.</span></span> 
-   <span data-ttu-id="9baa6-182">**Design evaluations, marketing and consumer research:** Eye tracking enables commercial companies to perform marketing and consumer studies in real-world environments or analyze what captures a user’s attention to improve product or space design.</span><span class="sxs-lookup"><span data-stu-id="9baa6-182">**Design evaluations, marketing and consumer research:** Eye tracking enables commercial companies to perform marketing and consumer studies in real-world environments or analyze what captures a user’s attention to improve product or space design.</span></span> 

### <a name="additional-use-cases"></a><span data-ttu-id="9baa6-183">추가 사용 사례</span><span class="sxs-lookup"><span data-stu-id="9baa6-183">Additional use cases</span></span>
- <span data-ttu-id="9baa6-184">**Gaming:** Ever wanted to have superpowers?</span><span class="sxs-lookup"><span data-stu-id="9baa6-184">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="9baa6-185">여기서 그 기회를 얻을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-185">Here's your chance!</span></span> <span data-ttu-id="9baa6-186">You can levitate holograms by staring at them.</span><span class="sxs-lookup"><span data-stu-id="9baa6-186">You can levitate holograms by staring at them.</span></span> <span data-ttu-id="9baa6-187">Shoot laser beams from your eyes - try it out in [RoboRaid for HoloLens 2](https://www.microsoft.com/p/roboraid/9nblggh5fv3j).</span><span class="sxs-lookup"><span data-stu-id="9baa6-187">Shoot laser beams from your eyes - try it out in [RoboRaid for HoloLens 2](https://www.microsoft.com/p/roboraid/9nblggh5fv3j).</span></span>
<span data-ttu-id="9baa6-188">Turn enemies into stone or freeze them.</span><span class="sxs-lookup"><span data-stu-id="9baa6-188">Turn enemies into stone or freeze them.</span></span> <span data-ttu-id="9baa6-189">X-광선을 사용해서 건물을 투시하세요.</span><span class="sxs-lookup"><span data-stu-id="9baa6-189">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="9baa6-190">상상하는 만큼 이루어집니다.</span><span class="sxs-lookup"><span data-stu-id="9baa6-190">Your imagination is the limit!</span></span>
<span data-ttu-id="9baa6-191">Beware of not overwhelming the user though - to find out more, check out our [eye-gaze-based input design guidelines](eye-gaze-interaction.md).</span><span class="sxs-lookup"><span data-stu-id="9baa6-191">Beware of not overwhelming the user though - to find out more, check out our [eye-gaze-based input design guidelines](eye-gaze-interaction.md).</span></span>

- <span data-ttu-id="9baa6-192">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking data to animate the avatar's eyes that indicate what the user is looking at.</span><span class="sxs-lookup"><span data-stu-id="9baa6-192">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking data to animate the avatar's eyes that indicate what the user is looking at.</span></span> 

- <span data-ttu-id="9baa6-193">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span><span class="sxs-lookup"><span data-stu-id="9baa6-193">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span></span> 

<br>

## <a name="using-eye-gaze-for-interaction"></a><span data-ttu-id="9baa6-194">Using eye-gaze for interaction</span><span class="sxs-lookup"><span data-stu-id="9baa6-194">Using eye-gaze for interaction</span></span>
<span data-ttu-id="9baa6-195">Building an interaction that takes advantage of fast-moving eye targeting can be challenging.</span><span class="sxs-lookup"><span data-stu-id="9baa6-195">Building an interaction that takes advantage of fast-moving eye targeting can be challenging.</span></span>
<span data-ttu-id="9baa6-196">On the one hand, the eyes move so fast that you need to be careful on how to use eye-gaze input, because otherwise users may find the experience overwhelming or distracting.</span><span class="sxs-lookup"><span data-stu-id="9baa6-196">On the one hand, the eyes move so fast that you need to be careful on how to use eye-gaze input, because otherwise users may find the experience overwhelming or distracting.</span></span> <span data-ttu-id="9baa6-197">On the other hand, you can also create truly magical experiences that will excite your users!</span><span class="sxs-lookup"><span data-stu-id="9baa6-197">On the other hand, you can also create truly magical experiences that will excite your users!</span></span> <span data-ttu-id="9baa6-198">To help you, check out our overview of key advantages, challenges and design recommendations for [eye-gaze for interaction](eye-gaze-interaction.md).</span><span class="sxs-lookup"><span data-stu-id="9baa6-198">To help you, check out our overview of key advantages, challenges and design recommendations for [eye-gaze for interaction](eye-gaze-interaction.md).</span></span> 
 
## <a name="fallback-solutions-when-eye-tracking-is-not-available"></a><span data-ttu-id="9baa6-199">Fallback solutions when eye tracking is not available</span><span class="sxs-lookup"><span data-stu-id="9baa6-199">Fallback solutions when eye tracking is not available</span></span>

<span data-ttu-id="9baa6-200">In rare cases, eye tracking data might not be available.</span><span class="sxs-lookup"><span data-stu-id="9baa6-200">In rare cases, eye tracking data might not be available.</span></span>
<span data-ttu-id="9baa6-201">This can be due to different reasons from which the most common are listed below:</span><span class="sxs-lookup"><span data-stu-id="9baa6-201">This can be due to different reasons from which the most common are listed below:</span></span>
* <span data-ttu-id="9baa6-202">The system failed to [calibrate the user](calibration.md).</span><span class="sxs-lookup"><span data-stu-id="9baa6-202">The system failed to [calibrate the user](calibration.md).</span></span>
* <span data-ttu-id="9baa6-203">The user skipped the [calibration](calibration.md).</span><span class="sxs-lookup"><span data-stu-id="9baa6-203">The user skipped the [calibration](calibration.md).</span></span>   
* <span data-ttu-id="9baa6-204">The user is calibrated, but decided to not give permission to your app to use their eye tracking data.</span><span class="sxs-lookup"><span data-stu-id="9baa6-204">The user is calibrated, but decided to not give permission to your app to use their eye tracking data.</span></span>    
* <span data-ttu-id="9baa6-205">The user has unique eyeglasses or some eye condition that the system does not yet support.</span><span class="sxs-lookup"><span data-stu-id="9baa6-205">The user has unique eyeglasses or some eye condition that the system does not yet support.</span></span>    
* <span data-ttu-id="9baa6-206">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes.</span><span class="sxs-lookup"><span data-stu-id="9baa6-206">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes.</span></span>   

<span data-ttu-id="9baa6-207">Hence, developers should ensure that there is appropriate fallback support for these users.</span><span class="sxs-lookup"><span data-stu-id="9baa6-207">Hence, developers should ensure that there is appropriate fallback support for these users.</span></span> <span data-ttu-id="9baa6-208">On the [Eye Tracking in DirectX](gaze-in-directx.md#fallback-when-eye-tracking-is-not-available) page, we explain the APIs required to detect whether eye tracking data is available.</span><span class="sxs-lookup"><span data-stu-id="9baa6-208">On the [Eye Tracking in DirectX](gaze-in-directx.md#fallback-when-eye-tracking-is-not-available) page, we explain the APIs required to detect whether eye tracking data is available.</span></span> 

<span data-ttu-id="9baa6-209">While some users may have consciously decided to revoke access to their eye tracking data and are ok with the trade-off of an inferior user experience to the privacy of not providing access to their eye tracking data, in some cases this may be unintentional.</span><span class="sxs-lookup"><span data-stu-id="9baa6-209">While some users may have consciously decided to revoke access to their eye tracking data and are ok with the trade-off of an inferior user experience to the privacy of not providing access to their eye tracking data, in some cases this may be unintentional.</span></span>  
<span data-ttu-id="9baa6-210">Hence, if your app uses eye tracking, and this is an important part of the experience, we recommend clearly communicating this to the user.</span><span class="sxs-lookup"><span data-stu-id="9baa6-210">Hence, if your app uses eye tracking, and this is an important part of the experience, we recommend clearly communicating this to the user.</span></span>     
<span data-ttu-id="9baa6-211">Kindly informing the user why eye tracking is critical for your application (maybe even listing some enhanced features) to experience the full potential of your application, can help the user to better understand what they are giving up.</span><span class="sxs-lookup"><span data-stu-id="9baa6-211">Kindly informing the user why eye tracking is critical for your application (maybe even listing some enhanced features) to experience the full potential of your application, can help the user to better understand what they are giving up.</span></span>   
<span data-ttu-id="9baa6-212">Help the user identify why eye tracking may not be working (based on the above checks) and offer some suggestions to quickly troubleshoot potential issues.</span><span class="sxs-lookup"><span data-stu-id="9baa6-212">Help the user identify why eye tracking may not be working (based on the above checks) and offer some suggestions to quickly troubleshoot potential issues.</span></span>     
<span data-ttu-id="9baa6-213">For example, if you can detect that the system supports eye tracking, the user is calibrated and has even given their permission, yet no eye tracking data is received, then this may point to some other issues such as smudges or the eyes being occluded.</span><span class="sxs-lookup"><span data-stu-id="9baa6-213">For example, if you can detect that the system supports eye tracking, the user is calibrated and has even given their permission, yet no eye tracking data is received, then this may point to some other issues such as smudges or the eyes being occluded.</span></span>    
<span data-ttu-id="9baa6-214">Please note that there are rare cases of users for whom eye tracking may simply not work.</span><span class="sxs-lookup"><span data-stu-id="9baa6-214">Please note that there are rare cases of users for whom eye tracking may simply not work.</span></span>   
<span data-ttu-id="9baa6-215">Hence, please be respectful of that by allowing to dismiss or even disable reminders for enabling eye tracking in your app.</span><span class="sxs-lookup"><span data-stu-id="9baa6-215">Hence, please be respectful of that by allowing to dismiss or even disable reminders for enabling eye tracking in your app.</span></span>

### <a name="fallback-for-apps-using-eye-gaze-as-a-primary-input-pointer"></a><span data-ttu-id="9baa6-216">Fallback for apps using eye-gaze as a primary input pointer</span><span class="sxs-lookup"><span data-stu-id="9baa6-216">Fallback for apps using eye-gaze as a primary input pointer</span></span>
<span data-ttu-id="9baa6-217">If your app uses eye-gaze as a pointer input to quickly select holograms across the scene, yet eye tracking data is unavailable, we recommend falling back to head-gaze and start showing the head-gaze cursor.</span><span class="sxs-lookup"><span data-stu-id="9baa6-217">If your app uses eye-gaze as a pointer input to quickly select holograms across the scene, yet eye tracking data is unavailable, we recommend falling back to head-gaze and start showing the head-gaze cursor.</span></span> <span data-ttu-id="9baa6-218">We recommend using a timeout (e.g., 500–1500 ms) to determine whether to switch or not.</span><span class="sxs-lookup"><span data-stu-id="9baa6-218">We recommend using a timeout (e.g., 500–1500 ms) to determine whether to switch or not.</span></span> <span data-ttu-id="9baa6-219">This action prevents cursors from appearing every time the system may briefly lose tracking due to fast eye motions or winks and blinks.</span><span class="sxs-lookup"><span data-stu-id="9baa6-219">This action prevents cursors from appearing every time the system may briefly lose tracking due to fast eye motions or winks and blinks.</span></span> <span data-ttu-id="9baa6-220">If you are a Unity developer, the automatic fallback to head-gaze is already handled in the Mixed Reality Toolkit.</span><span class="sxs-lookup"><span data-stu-id="9baa6-220">If you are a Unity developer, the automatic fallback to head-gaze is already handled in the Mixed Reality Toolkit.</span></span> <span data-ttu-id="9baa6-221">If you are a DirectX developer, you need to handle this switch yourself.</span><span class="sxs-lookup"><span data-stu-id="9baa6-221">If you are a DirectX developer, you need to handle this switch yourself.</span></span>

### <a name="fallback-for-other-eye-tracking-specific-applications"></a><span data-ttu-id="9baa6-222">Fallback for other eye-tracking-specific applications</span><span class="sxs-lookup"><span data-stu-id="9baa6-222">Fallback for other eye-tracking-specific applications</span></span>
<span data-ttu-id="9baa6-223">Your app may use eye-gaze in a unique way that is tailored specifically to the eyes.</span><span class="sxs-lookup"><span data-stu-id="9baa6-223">Your app may use eye-gaze in a unique way that is tailored specifically to the eyes.</span></span> <span data-ttu-id="9baa6-224">For example, animating an avatar’s eyes or for eye-based attention heatmaps relying on precise information about visual attention.</span><span class="sxs-lookup"><span data-stu-id="9baa6-224">For example, animating an avatar’s eyes or for eye-based attention heatmaps relying on precise information about visual attention.</span></span> <span data-ttu-id="9baa6-225">In this case, there is no clear fallback.</span><span class="sxs-lookup"><span data-stu-id="9baa6-225">In this case, there is no clear fallback.</span></span> <span data-ttu-id="9baa6-226">If eye tracking is not available, these capabilities may simply need to be disabled.</span><span class="sxs-lookup"><span data-stu-id="9baa6-226">If eye tracking is not available, these capabilities may simply need to be disabled.</span></span>
<span data-ttu-id="9baa6-227">Again, we recommend to clearly communicate this to the user who may be unaware that the capability is not working.</span><span class="sxs-lookup"><span data-stu-id="9baa6-227">Again, we recommend to clearly communicate this to the user who may be unaware that the capability is not working.</span></span>

<br>

<span data-ttu-id="9baa6-228">This page has hopefully provided you with a good overview to get you started understanding the role of eye tracking and eye-gaze input for HoloLens 2.</span><span class="sxs-lookup"><span data-stu-id="9baa6-228">This page has hopefully provided you with a good overview to get you started understanding the role of eye tracking and eye-gaze input for HoloLens 2.</span></span> <span data-ttu-id="9baa6-229">To get started developing, check out our information on the role of [eye-gaze for interacting with holograms](eye-gaze-interaction.md), [eye-gaze in Unity](https://aka.ms/mrtk-eyes) and [eye-gaze in DirectX](gaze-in-directx.md).</span><span class="sxs-lookup"><span data-stu-id="9baa6-229">To get started developing, check out our information on the role of [eye-gaze for interacting with holograms](eye-gaze-interaction.md), [eye-gaze in Unity](https://aka.ms/mrtk-eyes) and [eye-gaze in DirectX](gaze-in-directx.md).</span></span>


## <a name="see-also"></a><span data-ttu-id="9baa6-230">참고 항목</span><span class="sxs-lookup"><span data-stu-id="9baa6-230">See also</span></span>
* [<span data-ttu-id="9baa6-231">조정</span><span class="sxs-lookup"><span data-stu-id="9baa6-231">Calibration</span></span>](calibration.md)
* [<span data-ttu-id="9baa6-232">편안함</span><span class="sxs-lookup"><span data-stu-id="9baa6-232">Comfort</span></span>](comfort.md)
* [<span data-ttu-id="9baa6-233">시선 응시 기반 상호 작용</span><span class="sxs-lookup"><span data-stu-id="9baa6-233">Eye-gaze-based interaction</span></span>](eye-gaze-interaction.md)
* [<span data-ttu-id="9baa6-234">Eye-gaze in DirectX</span><span class="sxs-lookup"><span data-stu-id="9baa6-234">Eye-gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="9baa6-235">Eye-gaze in Unity (Mixed Reality Toolkit)</span><span class="sxs-lookup"><span data-stu-id="9baa6-235">Eye-gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="9baa6-236">응시 및 커밋</span><span class="sxs-lookup"><span data-stu-id="9baa6-236">Gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="9baa6-237">음성 입력 </span><span class="sxs-lookup"><span data-stu-id="9baa6-237">Voice input</span></span>](voice-design.md)


