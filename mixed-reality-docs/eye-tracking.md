---
title: 시선 추적
description: 시선 추적
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: Eye Tracking, Mixed Reality, Input, Eye Gaze
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 06/05/2019
ms.locfileid: "66453695"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="aa6ae-104">HoloLens 2의 시선 추적</span><span class="sxs-lookup"><span data-stu-id="aa6ae-104">Eye tracking on HoloLens 2</span></span>
<span data-ttu-id="aa6ae-105">HoloLens 2에서는 개발자에게 사용자가 바라보는 대상에 대한 정보를 사용할 수 있는 뛰어난 능력을 제공하여 홀로그래픽 환경에서 상황 및 인간을 완전히 새로운 차원으로 이해할 수 있도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-105">HoloLens 2 allows for a whole new level of context and human understanding within the holographic experience by providing developers with the incredible ability of using information about what users are looking at.</span></span> <span data-ttu-id="aa6ae-106">이 페이지에서는 개발자가 다양한 사용 사례에서 시선 추적을 활용하는 방법과 시선 응시 기반 사용자 인터페이스를 디자인할 때 고려해야 할 사항을 간단히 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-106">This page gives an overview of how developers can benefit from eye tracking for various use cases and what to look out for when designing eye-gaze-based user interfaces.</span></span> 

## <a name="use-cases"></a><span data-ttu-id="aa6ae-107">사용 사례</span><span class="sxs-lookup"><span data-stu-id="aa6ae-107">Use cases</span></span>
<span data-ttu-id="aa6ae-108">시선 추적을 사용하여 애플리케이션에서는 사용자가 실시간으로 보는 곳을 추적할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-108">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="aa6ae-109">이 섹션에서는 잠재적인 사용 사례와 혼합 현실에서 시선 추적을 통해 구현할 수 있는 새로운 상호 작용에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-109">This section describes some of the potential use cases and novel interactions that become possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="aa6ae-110">시작하기 전에 집고 넘어갈 사항은 다음 내용에서 [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)을 여러 번 언급한다는 것입니다. 빠르고 손쉬운 시선 지원 대상 선택, 사용자가 바라보는 위치에 따라 텍스트 자동 스크롤과 같이 시선 추적 기능이 사용되는 흥미롭고 강력한 예제를 제공하기 때문입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-110">Before getting started, in the following we will mention the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) several times as it provides several interesting and powerful examples for using eye tracking such as quick and effortless eye-supported target selections and automatically scrolling through text based on where the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="aa6ae-111">사용자 의도</span><span class="sxs-lookup"><span data-stu-id="aa6ae-111">User intent</span></span>    
<span data-ttu-id="aa6ae-112">사용자가 바라보는 위치에 대한 정보는 음성, 손 및 컨트롤러 등의 **다른 입력에 대한 강력한 컨텍스트**를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-112">Information about where a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="aa6ae-113">이러한 컨텍스트를 다양한 작업에서 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-113">This can be used for various tasks.</span></span>
<span data-ttu-id="aa6ae-114">예를 들어, 간단히 홀로그램을 바라보고 “select”라고 말하여 빠르고 손쉽게 **타기팅**하거나([머리 응시 및 커밋](gaze-and-commit.md) 참조) "put this..."라고 말한 다음, 홀로그램을 표시할 위치를 바라보고 “there”라고 말할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-114">For example, this may range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where you want to place the hologram and say "...there".</span></span> <span data-ttu-id="aa6ae-115">이에 대한 예는 [Mixed Reality Toolkit - 시선 지원 대상 선택](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) 및 [Mixed Reality Toolkit - 시선 지원 대상 위치 지정](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)에서 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-115">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="aa6ae-116">사용자 의도의 추가 예로 사용자가 바라보는 대상에 대한 정보를 사용하여 포함된 가상 에이전트 및 대화형 홀로그램의 효과를 향상시키는 경우가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-116">An additional example for user intent may include using information about what users look at to enhance the engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="aa6ae-117">예를 들어, 가상 에이전트는 현재 본 콘텐츠를 기준으로 사용 가능한 옵션 및 해당 동작을 조정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-117">For example, virtual agents may adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="aa6ae-118">암시적 작업</span><span class="sxs-lookup"><span data-stu-id="aa6ae-118">Implicit actions</span></span>
<span data-ttu-id="aa6ae-119">암시적 작업의 범주는 사용자 의도와 밀접하게 관련되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-119">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="aa6ae-120">즉, 홀로그램이나 사용자 인터페이스 요소는 사용자가 시스템과 상호 작용한다고 느끼지 못하며, 오히려 시스템과 사용자가 동기화되어 있다고 생각할 수 있는 직관적 방식으로 반응합니다. 예를 들어, 성공적인 한 가지 예는 **시선 응시 기반 자동 스크롤**입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-120">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like you are interacting with the system at all, but rather that the system and the user are in sync. For example, one immensely successful example is **eye-gaze-based auto scroll**.</span></span> <span data-ttu-id="aa6ae-121">해당 개념은 다음과 같이 간단합니다. 사용자는 텍스트를 읽으며, 계속해서 읽을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-121">The idea is as simple: The user reads a text and can just keep on reading.</span></span> <span data-ttu-id="aa6ae-122">사용자가 읽기 흐름을 유지할 수 있도록 텍스트가 서서히 위로 이동합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-122">The text gradually moves up to keep users in their reading flow.</span></span> <span data-ttu-id="aa6ae-123">주요 측면은 스크롤 속도가 사용자의 읽기 속도에 맞게 조정된다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-123">A key aspect is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="aa6ae-124">또 다른 예로 사용자가 집중하고 있는 대상을 정확히 바라보고 있다고 느낄 수 있게 하는 **시선 지원 확대/축소 및 이동**이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-124">Another example is **eye-supported zoom and pan** for which the user can feel like diving exactly toward what he or she is focusing at.</span></span> <span data-ttu-id="aa6ae-125">확대/축소 트리거 및 확대/축소 속도 제어는 제어한다는 느낌을 주는 데 중요한 음성 또는 손 입력을 통해 제어할 수 있으며, 사용자가 작동의 어려움을 느끼지 않도록 합니다. 이러한 디자인 지침에 대해서는 아래에서 좀 더 자세히 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-125">Triggering the zoom and controlling the zoom speed can be controlled via voice or hand input which is important about providing the feeling of control and avoid overwhelming the user (we will talk about these design guidelines in more detail below).</span></span> <span data-ttu-id="aa6ae-126">예를 들어, 일단 확대하고 나면, 사용자는 시선 응시만으로 거리를 원활하게 따라가면서 인접 환경을 살펴볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-126">Once zoomed in, the user can then smoothly follow, for example, the course of a street to explore his or her neighborhood just simply by using their eye gaze.</span></span>
<span data-ttu-id="aa6ae-127">이러한 유형의 상호 작용을 나타내는 데모 예제는 [Mixed Reality Toolkit - 시선 지원 탐색](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) 샘플에서 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-127">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="aa6ae-128">_암시적 작업_의 추가 사용 사례로는 다음이 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-128">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="aa6ae-129">**스마트 알림:** 집중해서 보고 있는 위치에 알림이 계속 표시되어 짜증이 난적이 있나요?</span><span class="sxs-lookup"><span data-stu-id="aa6ae-129">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="aa6ae-130">사용자가 현재 집중하고 있는 위치를 고려해서 더 나은 환경을 구현할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-130">Taking into account where a user is currently paying attention to, you can make it better!</span></span> <span data-ttu-id="aa6ae-131">사용자가 현재 보고 있는 위치에서의 알림 오프셋을 표시하여 방해 요소를 제한하고, 다 읽은 후에는 자동으로 해제되도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-131">Show notifications offset from where the user is currently looking to limit distractions and automatically dismiss them once finished reading.</span></span> 
- <span data-ttu-id="aa6ae-132">**주의 홀로그램:** 바라볼 때 미세하게 반응하는 홀로그램입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-132">**Attentive holograms:** Holograms that subtly react when being looked at.</span></span> <span data-ttu-id="aa6ae-133">이러한 홀로그램은 약간 빛나는 UI 요소, 느리게 피는 꽃부터 사용자가 한참 응시한 후에 사용자를 다시 바라보기 시작하거나 사용자의 시선 응시를 피하려고 하는 가상의 애완동물까지 다양할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-133">This may range from slightly glowing UI elements, a slowly blooming flower to a virtual pet starting to look back at you or trying to avoid your eye gaze after a prolonged stare.</span></span> <span data-ttu-id="aa6ae-134">이러한 홀로그램은 앱에 재미있는 연결성 및 만족도를 제공할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-134">This may provide an interesting sense of connectivity and satisfaction in your app.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="aa6ae-135">주의 추적</span><span class="sxs-lookup"><span data-stu-id="aa6ae-135">Attention tracking</span></span>   
<span data-ttu-id="aa6ae-136">사용자가 바라보는 위치에 대한 정보는 디자인의 유용성을 평가하고 효율적인 작업 흐름의 문제를 식별하는 매우 강력한 도구입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-136">Information about where users look at is an immensely powerful tool to assess usability of designs and to identify problems in efficient work streams.</span></span> <span data-ttu-id="aa6ae-137">현재, 시선 추적 시각화 및 분석은 다양한 응용 분야에서 이미 일반적으로 사용되고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-137">By now,  eye tracking visualization and analytics are already a common practice in various application areas.</span></span> <span data-ttu-id="aa6ae-138">HoloLens 2에서는 3D 홀로그램을 실제 상황에 배치하고 그에 따라 평가할 수 있으므로 이해의 폭이 훨씬 더 넓어질 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-138">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed alongside.</span></span> <span data-ttu-id="aa6ae-139">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)은 시선 추적 데이터를 기록 및 로드하고 시각화하는 방법의 기본적인 예를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-139">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and for how to visualize them.</span></span>

<span data-ttu-id="aa6ae-140">이 영역의 다른 응용 분야로 다음이 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-140">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="aa6ae-141">**원격 시선 응시 시각화:** 예를 들어, 원격 공동 작업자가 바라보는 대상을 시각화하여 지침이 올바르게 이해되고 준수되는지 여부를 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-141">**Remote eye gaze visualization:** Visualize what remote collaborators are looking at to, for example, ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="aa6ae-142">**사용자 연구:** 주의 추적을 사용하여 초보 사용자 및 전문가가 콘텐츠를 시각적으로 분석하는 방식이나 복잡한 작업(예: 의료 데이터 분석 또는 기계 작동)에 대한 손-시선 조정 방법을 알아볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-142">**User research studies:** Attention tracking can be used to explore the way novice vs. experts users visually analyze content or their hand-eye-coordination for complex tasks (e.g., for analysis of medical data or while operating machinery).</span></span>
-   <span data-ttu-id="aa6ae-143">**학습 시뮬레이션 및 성능 모니터링:** 작업 실행을 연습하면서 실행 흐름의 병목 상태를 보다 효과적으로 식별하고 작업 실행을 최적화합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-143">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="aa6ae-144">**평가, 광고 및 마케팅 리서치 디자인:** 시선 추적은 마켓 리서치에서 웹 사이트 및 제품 디자인을 평가하는 데 사용하는 일반적인 도구입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-144">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research to evaluate website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="aa6ae-145">추가 사용 사례</span><span class="sxs-lookup"><span data-stu-id="aa6ae-145">Additional use cases</span></span>
- <span data-ttu-id="aa6ae-146">**게임:** 초능력을 갖고 싶었던 적이 있나요?</span><span class="sxs-lookup"><span data-stu-id="aa6ae-146">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="aa6ae-147">여기서 그 기회를 얻을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-147">Here's your chance!</span></span> <span data-ttu-id="aa6ae-148">홀로그램을 응시하여 공중으로 올려 보내보세요.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-148">Levitate holograms by staring at them.</span></span> <span data-ttu-id="aa6ae-149">눈에서 레이저 광선을 쏘세요.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-149">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="aa6ae-150">적을 돌로 만들거나 얼려버리세요.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-150">Turn enemies into stone or freeze them!</span></span> <span data-ttu-id="aa6ae-151">X-광선을 사용해서 건물을 투시하세요.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-151">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="aa6ae-152">상상하는 만큼 이루어집니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-152">Your imagination is the limit!</span></span>  

- <span data-ttu-id="aa6ae-153">**표현적 아바타:** 시선 추적은 실시간 시선 추적 데이터를 사용하여 아바타의 눈이 사용자가 현재 보고 있는 대상을 나타내도록 애니메이트하여 보다 표현적인 3D 아바타를 구현할 수 있도록 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-153">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking date to animate the avatar's eyes to indicate what the user is currently looking at.</span></span> <span data-ttu-id="aa6ae-154">또한 윙크 및 깜박임을 추가하여 더 많은 표현을 추가합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-154">It also adds more expressiveness by adding winks and blinks.</span></span> 

- <span data-ttu-id="aa6ae-155">**텍스트 입력:** 특히, 음성 또는 손을 사용하기 불편한 경우에 시선 추적을 사용하여 손쉽게 텍스트를 입력할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-155">**Text entry:** Eye tracking can be used as an interesting alternative for low-effort text entry especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="aa6ae-156">시선 추적 API</span><span class="sxs-lookup"><span data-stu-id="aa6ae-156">Eye tracking API</span></span>
<span data-ttu-id="aa6ae-157">시선 응시 조작을 위한 특정 디자인 지침을 자세히 알아보기 전에 HoloLens 2 아이 트래커가 제공하는 기능을 간단히 살펴보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-157">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point to the capabilities that the HoloLens 2 Eye Tracker is providing.</span></span> <span data-ttu-id="aa6ae-158">[시선 추적 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)는 `Windows.Perception.People.EyesPose`를 통해 액세스할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-158">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: `Windows.Perception.People.EyesPose`.</span></span> <span data-ttu-id="aa6ae-159">이 기능은 개발자에게 단일 시선 응시 레이(응시 원점 및 방향)를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-159">It provides a single eye gaze ray (gaze origin and direction) to developers.</span></span>
<span data-ttu-id="aa6ae-160">아이 트래커는 _30 FPS_에 대한 데이터를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-160">The eye tracker provides data at about _30 FPS_.</span></span>
<span data-ttu-id="aa6ae-161">예상 시선 응시는 실제로 바라본 대상 주변에서 시각적 각도로 ca.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-161">The predicted eye gaze lies within ca.</span></span> <span data-ttu-id="aa6ae-162">1.0 - 1.5도 이내에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-162">1.0 - 1.5 degrees in visual angle around the actual looked at target.</span></span> <span data-ttu-id="aa6ae-163">약간의 부정확성이 예상되므로, 이 하한 값에 대해 약간의 여유를 계획해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-163">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="aa6ae-164">이 내용은 아래에서 좀 더 자세히 살펴보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-164">We will discuss this more below.</span></span> <span data-ttu-id="aa6ae-165">시선 추적이 정확히 작동하려면 각 사용자가 시선 추적 사용자 보정을 진행해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-165">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="aa6ae-166">![2m 거리에서 최적 대상 크기](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="aa6ae-166">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="aa6ae-167">*2m 거리에서 최적 대상 크기*</span><span class="sxs-lookup"><span data-stu-id="aa6ae-167">*Optimal target size at 2 meter distance*</span></span>


## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="aa6ae-168">시선 응시 디자인 지침</span><span class="sxs-lookup"><span data-stu-id="aa6ae-168">Eye gaze design guidelines</span></span>
<span data-ttu-id="aa6ae-169">빠르게 이동하는 시선 타기팅을 활용하는 상호 작용을 구축하는 작업은 어려울 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-169">Building an interaction that takes advantage of fast moving eye targeting can be challenging.</span></span> <span data-ttu-id="aa6ae-170">이 섹션에서는 앱을 디자인할 때 고려해야 하는 주요 장점 및 문제점을 요약해서 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-170">In this section, we summarize the key advantages and challenges to take into account when designing your app.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="aa6ae-171">시선 응시 입력의 이점</span><span class="sxs-lookup"><span data-stu-id="aa6ae-171">Benefits of eye gaze input</span></span>
- <span data-ttu-id="aa6ae-172">**고속 가리키기.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-172">**High speed pointing.**</span></span> <span data-ttu-id="aa6ae-173">눈 근육은 우리 몸에서 가장 빠르게 반응하는 근육입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-173">The eye muscle is the fastest reacting muscle in our body.</span></span> 

- <span data-ttu-id="aa6ae-174">**적은 노력.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-174">**Low effort.**</span></span> <span data-ttu-id="aa6ae-175">신체 움직임은 거의 필요하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-175">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="aa6ae-176">**암시성.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-176">**Implicitness.**</span></span> <span data-ttu-id="aa6ae-177">종종 "독심술"이라고도 하는 사용자 시선 움직임에 대한 정보를 통해 시스템은 사용자가 교감하려는 대상을 알 수 있게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-177">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage with.</span></span> 

- <span data-ttu-id="aa6ae-178">**대체 입력 채널.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-178">**Alternative input channel.**</span></span> <span data-ttu-id="aa6ae-179">시선 응시는 손-시선을 조정하면서 사용자가 수년 동안 얻은 경험을 토대로 손 및 음성 입력을 보완하는 강력히 지원 입력을 제공할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-179">Eye gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="aa6ae-180">**시각적 주의.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-180">**Visual attention.**</span></span> <span data-ttu-id="aa6ae-181">또 다른 중요한 이점은 사용자가 주의하고 있는 대상을 유추할 수 있다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-181">Another important benefit is the possibility to infer what a user's is paying attention to.</span></span> <span data-ttu-id="aa6ae-182">이러한 이점은 여러 다른 디자인을 보다 효과적으로 평가하는 것부터 원격 커뮤니케이션을 위한 보다 스마트한 사용자 인터페이스와 향상된 소셜 신호를 지원하는 것까지 다양한 분야에서 응용될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-182">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter User Interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="aa6ae-183">분명히 말하지만, 시선 응시를 입력으로 사용하면 빠르고 간편한 상황별 신호를 제공할 수 있습니다. 이 방식을 *음성* 및 *수동* 입력과 같은 기타 입력 방식과 함께 사용하여 사용자 의도를 확실하게 나타낼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-183">In a nutshell, using eye gaze as an input potentially offers a fast and effortless contextual signal - This is particularly powerful in combination with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="aa6ae-184">시선 응시를 입력으로 사용할 때의 해결 과제</span><span class="sxs-lookup"><span data-stu-id="aa6ae-184">Challenges of eye gaze as an input</span></span>
<span data-ttu-id="aa6ae-185">큰 힘에는 큰 책임이 따릅니다. 시선 응시를 사용하여 슈퍼히어로와 같은 마법의 사용자 경험을 만들 수 있지만, 어떤 경우에 이러한 기능을 사용하는 것이 적절하지 않은지 파악하는 것이 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-185">With lots of power, comes lots of responsibility: While eye gaze can be used to create magical user experiences feeling like a superhero, it is also important to know what it is not good at to account for this appropriately.</span></span> <span data-ttu-id="aa6ae-186">다음에서는 고려해야 할 몇 가지 *해결 과제*와 시선 응시 입력을 사용할 때 이러한 문제를 해결하는 방법을 살펴봅니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-186">In the following, we discuss some *challenges* to take into account and how to address them when working with eye gaze input:</span></span> 

- <span data-ttu-id="aa6ae-187">**시선 응시는 "항상 켜져 있음"** 눈꺼풀을 여는 순간, 눈은 환경에 있는 사물을 응시하기 시작합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-187">**Your eye gaze is "always on"** The moment you open your eye lids, your eyes start fixating things in your environment.</span></span> <span data-ttu-id="aa6ae-188">사물을 너무 오랫동안 바라보았기 때문에 보이는 모든 것에 반응하고 실수로 행동을 하게 된다면 끔직한 일일 것입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-188">Reacting to every look you make and potentially accidentally issuing actions because you looked at something for too long would result in a terrible experience!</span></span>
<span data-ttu-id="aa6ae-189">이때문에 시선 응시를 *음성 명령*, *손 제스처*, *단추 클릭* 또는 연장된 바라보기와 결합하여 대상 선택을 트리거하는 것이 바람직한 것입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-189">This is why we recommend combining eye gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="aa6ae-190">또한 이 솔루션을 사용하면 사용자는 억지로 대상을 트리거한다는 느낌 없이 자유롭게 둘러볼 수 있는 모드가 구현됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-190">This solution also allows for a mode in which the user can freely look around without the overwhelming feeling of involuntarily triggering something.</span></span> <span data-ttu-id="aa6ae-191">이 이슈는 단순히 대상을 바라보면서 시각적 및 청각적 피드백을 디자인할 때도 고려해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-191">This issue should also be taken into account when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="aa6ae-192">즉각적인 팝아웃 효과 또는 호버 소리로 사용자가 당황하게 하지 않도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-192">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="aa6ae-193">미묘함이 핵심입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-193">Subtlety is key!</span></span> <span data-ttu-id="aa6ae-194">디자인 권장 사항에 대해 논의하면서 아래에서 몇 가지 모범 사례를 다룰 예정입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-194">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="aa6ae-195">**관찰 및 제어** 사진을 벽에 정확하게 맞추려고 한다고 가정해 봅니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-195">**Observation vs. control** Imagine you want to precisely align a photograph at your wall.</span></span> <span data-ttu-id="aa6ae-196">벽에 맞는지 보기 위해 테두리와 주변을 보게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-196">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="aa6ae-197">이제 동시에 시선 응시를 입력으로 사용하여 사진을 이동하려는 경우라면 어떻게 할지 상상해보세요.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-197">Now imagine how you would do that when at the same time you want to use your eye gaze as an input to move the picture.</span></span> <span data-ttu-id="aa6ae-198">어렵죠. 그렇지 않나요?</span><span class="sxs-lookup"><span data-stu-id="aa6ae-198">Difficult, isn't it?</span></span> <span data-ttu-id="aa6ae-199">이러한 경우는 입력 및 제어가 모두 필요할 때 시선 응시의 이중 역할을 설명하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-199">This describes the double role of eye gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="aa6ae-200">**클릭하기 전에 나가기:** 연구에 따르면, 빠른 대상 선택을 위해서는 수동 클릭(예: 에어 탭)을 마무리하기 전에 사용자의 시선 응시가 계속 움직일 수 있음을 보여 주었습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-200">**Leave before click:** For quick target selections, research has shown that a user's eye gaze may move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="aa6ae-201">따라서 빠른 시선 응시 신호와 더 느린 제어 입력(예: 음성, 손, 컨트롤러)을 동기화하기 위해 특히 주의해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-201">Hence, special attention must be paid to synchronizing the fast eye gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="aa6ae-202">**작은 대상:** 텍스트를 읽으려고 하는 데 너무 작아서 쉽게 읽을 수 없다고 느껴지나요?</span><span class="sxs-lookup"><span data-stu-id="aa6ae-202">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to comfortably read?</span></span> <span data-ttu-id="aa6ae-203">이러한 느낌이 눈을 압박하면 초점을 잘 맞추기 위해 눈을 재조정하려고 하므로 피곤하게 느껴질 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-203">This straining feeling on your eyes that cause you to feel tired and worn out because you try to readjust your eyes to focus better?</span></span>
<span data-ttu-id="aa6ae-204">강제로 사용자들이 시선 타기팅을 사용해서 앱에 있는 너무 작은 대상을 선택하도록 하면 사용자들도 이러한 느낌을 받게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-204">This is a feeling you may invoke in your users when forcing them to select too small targets in your app using eye targeting.</span></span>
<span data-ttu-id="aa6ae-205">사용자를 위해 기분 좋고 편리한 환경을 디자인하려면 대상에 대한 시각적 각도를 2° 이상으로 유지하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-205">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="aa6ae-206">**불규칙한 시선 응시 움직임** 눈은 응시할 때마다 빠르게 움직입니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-206">**Ragged eye gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="aa6ae-207">기록된 시선 움직임의 검색 경로를 바라보면 불규칙하다는 것을 알 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-207">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="aa6ae-208">시선은 *머리 응시* 또는 *손 모션*과 비교할 때 빠르게 저절로 이동합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-208">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="aa6ae-209">**추적 안정성:** 빛이 달라지면 눈이 새로운 상황에 적응해야 하므로 시선 추적 정확도가 약간 저하될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-209">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="aa6ae-210">이러한 정확도는 위에서 언급된 2° 한도 내에서 달라지므로 앱 디자인에 반드시 영향을 미친다고 볼 수는 없습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-210">While this should not necessarily affect your app design, as the accuracy should be within the above mentioned limitation of 2°.</span></span> <span data-ttu-id="aa6ae-211">즉, 사용자가 다른 보정을 진행해야 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-211">It may mean that the user has to run another calibration.</span></span> 


### <a name="design-recommendations"></a><span data-ttu-id="aa6ae-212">디자인 권장 사항</span><span class="sxs-lookup"><span data-stu-id="aa6ae-212">Design recommendations</span></span>
<span data-ttu-id="aa6ae-213">다음에서는 시선 응시 입력의 기술된 이점과 해결 과제를 토대로 구체적인 디자인 권장 사항을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-213">In the following, we list specific design recommendations based on the described advantages and challenges for eye gaze input:</span></span>

1. <span data-ttu-id="aa6ae-214">**시선 응시 != 머리 응시:**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-214">**Eye gaze != Head gaze:**</span></span>
    - <span data-ttu-id="aa6ae-215">**빠르지만 불규칙한 시선 움직임이 입력 작업에 잘 맞는지 여부를 고려합니다.** 빠르고 불규칙한 시선 움직임은 시야각 너머의 대상을 빠르게 선택하는 데 유용하지만, 매끄러운 입력 궤적(예: 그림 그리기 또는 주석 둘레에 원 그리기)이 필요한 작업에는 적절하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-215">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great to quickly select targets across our Field of View, it is less applicable for tasks that require smooth input trajectories (e.g., for drawing or encircling annotations).</span></span> <span data-ttu-id="aa6ae-216">이 경우 손 또는 머리 가리키기가 더 나을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-216">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="aa6ae-217">**사용자의 시선 응시에 슬라이더나 커서 등을 직접적으로 연결하지 않도록 합니다.**</span><span class="sxs-lookup"><span data-stu-id="aa6ae-217">**Avoid attaching something directly to the user’s eye gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="aa6ae-218">커서의 경우 투영된 시선 응시 신호에서 약간의 오프셋이 발생하므로 "커서가 달아나는" 결과가 나타날 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-218">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye gaze signal.</span></span> <span data-ttu-id="aa6ae-219">슬라이더의 경우 개체가 올바른 위치인지 여부를 확인하면서 시선으로 슬라이더를 제어하려고 하므로 충돌이 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-219">In case of a slider, it conflicts with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="aa6ae-220">간단히 말해서 사용자는 제어하기가 너무 어렵다고 느낄 수 있으며, 해당 사용자에게 신호가 정확하지 않을 경우 이러한 어려움이 두드러집니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-220">In a nutshell, users may quickly feel overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="aa6ae-221">**시선 응시와 다른 입력 조합:** 손 제스처, 음성 명령 또는 단추 누르기와 같은 다른 입력과 시선 추적을 통합하면 다음과 같은 몇 가지 이점을 얻을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-221">**Combine eye gaze with other inputs:** The integration of Eye Tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="aa6ae-222">**자유로운 관찰 허용:** 시선의 주 역할이 환경을 관찰하는 것이라면, 사용자가 (시각적, 청각적 등) 피드백이나 작업을 트리거하지 않으면서 둘러볼 수 있는 것이 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-222">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important to allow users to look around without triggering any (visual, auditory, ...) feedback or actions.</span></span> 
    <span data-ttu-id="aa6ae-223">ET를 다른 입력 컨트롤과 조합하면 ET 관찰과 입력 제어 모드 간을 원활하게 전환할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-223">Combining ET with another input control allows for smoothly transitioning between ET observation and input control modes.</span></span>
  
    - <span data-ttu-id="aa6ae-224">**강력한 컨텍스트 공급자:** 음성 명령을 발화하거나 손 제스처를 수행하면서 사용자가 바라보는 위치에 대한 정보를 사용하면 시야각을 넘어 입력을 간편하게 연결할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-224">**Powerful context provider:** Using information about where the user is looking at while uttering a voice command or performing a hand gesture allows for effortlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="aa6ae-225">예를 들면 다음과 같습니다. “거기에 배치”: 대상 및 목적지를 간단히 바라봄으로써 장면에서 빠르고 유연하게 홀로그램을 선택하여 배치합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-225">Examples include: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="aa6ae-226">**다중 모달 입력을 동기화할 필요(“클릭하기 전에 나가기” 이슈):** 더 복잡한 추가 입력(예: 긴 음성 명령 또는 손 제스처)과 빠른 시선 움직임을 조합하면 추가적인 입력 명령을 끝내기 전에 시선 응시로 인해 동작이 인식될 위험이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-226">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs (e.g., long voice commands or hand gestures) bears the risk of moving on with your eye gaze before finishing the additional input command.</span></span> <span data-ttu-id="aa6ae-227">따라서 고유한 입력 컨트롤(예: 사용자 지정 손 제스처)을 만드는 경우 이러한 입력의 시작이나 대략적인 기간을 로깅하여 사용자가 이전에 응시했던 대상과 연관짓도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-227">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had fixated on in the past.</span></span>
    
3. <span data-ttu-id="aa6ae-228">**시선 추적 입력에 대한 미묘한 피드백:** 대상을 바라보는 경우 피드백을 제공(시스템 의도한 대로 작동함을 나타내기 위해)하는 것이 유용하지만 미묘한 차이를 유지해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-228">**Subtle feedback for eye tracking input:** It is useful to provide feedback if a target is looked at (to indicate that the system is working as intended) but should be kept subtle.</span></span> <span data-ttu-id="aa6ae-229">여기에는 시각적 강조 표시를 느리게 블렌드 인/아웃하는 경우가 포함될 수 있고, 느린 모션(예: 대상을 서서히 늘림)과 같은 다른 미묘한 대상 동작을 수행하여 사용자의 현재 워크플로를 중단시키지 않으면서 사용자가 대상을 바라보고 있다는 사실을 시스템이 올바르게 감지하고 있음을 나타낼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-229">This may include slowly blending in/out visual highlights or perform other subtle target behaviors, such as slow motions (e.g., slightly increasing the target) to indicate that the system correctly detected that the user is looking at a target, however, without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="aa6ae-230">**부자연스러운 시선 움직임을 입력으로 적용하지 않음:** 앱에서 동작을 트리거하기 위해 강제로 사용자가 특정 눈 움직임(응시 제스처)을 수행하도록 하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-230">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your app.</span></span>

5. <span data-ttu-id="aa6ae-231">**부정확성 고려:** 사용자가 느낄 수 있는 부정확성은 오프셋 및 지터의 두 가지 유형으로 구분됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-231">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: Offset and Jitter.</span></span> <span data-ttu-id="aa6ae-232">오프셋을 해결하는 가장 쉬운 방법은 상호 작용할 충분히 큰 대상을 제공하는 것입니다(시각적 각도가 2°보다 큼. 참조로, 팔을 뻗을 때 엄지손톱의 시각적 각도는 약 2°임(1)).</span><span class="sxs-lookup"><span data-stu-id="aa6ae-232">The easiest way to address offsets is to provide sufficiently large targets to interact with (> 2° in visual angle – as reference: your thumbnail is about 2° in visual angle when you stretch out your arm (1)).</span></span> <span data-ttu-id="aa6ae-233">이에 따라 다음과 같은 지침이 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-233">This leads to the following guidance:</span></span>
    - <span data-ttu-id="aa6ae-234">강제로 사용자가 작은 대상을 선택하도록 하지 않습니다. 연구에 따르면, 대상이 충분히 클 경우(또한 시스템이 잘 디자인됨) 사용자는 조작을 마법처럼 쉬운 것으로 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-234">Do not force users to select tiny targets: Research has shown that if targets are sufficiently large (and the system is designed well), users describe the interaction as effortless and magical.</span></span> <span data-ttu-id="aa6ae-235">대상이 너무 작으면 조작 경험을 피곤하고 당황스러운 것으로 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="aa6ae-235">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="aa6ae-236">참고 항목</span><span class="sxs-lookup"><span data-stu-id="aa6ae-236">See also</span></span>
* [<span data-ttu-id="aa6ae-237">헤드 게이즈 및 커밋</span><span class="sxs-lookup"><span data-stu-id="aa6ae-237">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="aa6ae-238">DirectX의 헤드 및 눈 응시</span><span class="sxs-lookup"><span data-stu-id="aa6ae-238">Head and eye gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="aa6ae-239">Unity의 시선 응시(Mixed Reality Toolkit)</span><span class="sxs-lookup"><span data-stu-id="aa6ae-239">Eye gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="aa6ae-240">손 제스처</span><span class="sxs-lookup"><span data-stu-id="aa6ae-240">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="aa6ae-241">음성 입력 </span><span class="sxs-lookup"><span data-stu-id="aa6ae-241">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="aa6ae-242">모션 컨트롤러</span><span class="sxs-lookup"><span data-stu-id="aa6ae-242">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="aa6ae-243">편안함</span><span class="sxs-lookup"><span data-stu-id="aa6ae-243">Comfort</span></span>](comfort.md)
